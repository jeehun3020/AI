ğŸ§  Decision Tree 

ğŸ“˜ Overview

This project implements a Decision Tree classifier from scratch and evaluates its performance on real-world data.
Through this assignment, I aimed to understand how decision trees split data, measure uncertainty, and balance between model complexity and generalization.

â¸»

ğŸ“Š Dataset
	â€¢	Name: Breast Cancer Wisconsin (Diagnostic)
	â€¢	Source: UCI Machine Learning Repositoryï¿¼
	â€¢	Samples: 569
	â€¢	Features: 30 numeric features
	â€¢	Target:
	â€¢	0: Malignant
	â€¢	1: Benign
  
âš™ï¸ Implementation Details
Step
Description
1. Data Preprocessing
Standardized numeric features and encoded labels.
2. Splitting Criteria
Used Information Gain based on Entropy.
3. Tree Construction
Recursive binary splitting until purity or max depth.
4. Stopping Conditions
Minimum samples per node and maximum depth thresholds.
5. Evaluation
Calculated accuracy, confusion matrix, and visualized decision boundaries.

ğŸ“ˆ Results
Metric
Training
Test
Accuracy
0.97
0.94
Depth
5
â€”
	â€¢	The model performs well without significant overfitting.
	â€¢	The decision boundaries are interpretable and align with feature importance.
	â€¢	Compared to logistic regression, the tree offers higher interpretability.

â¸»
ğŸ§© Discussion
	â€¢	Strengths:
	â€¢	Simple and interpretable structure.
	â€¢	Handles nonlinear relationships automatically.
	â€¢	Weaknesses:
	â€¢	Sensitive to noise; prone to overfitting without pruning.
	â€¢	Small feature changes can lead to different tree structures.
	â€¢	Improvement Ideas:
	â€¢	Apply pruning or ensemble methods (e.g., Bagging or Random Forest).
	â€¢	Experiment with Gini Index as an alternative splitting criterion.

â¸»
ğŸ§® Key Equations

Entropy = - \sum_i p_i \log_2(p_i)

Information\ Gain = H(parent) - \sum_{children} \frac{n_{child}}{n_{parent}} H(child)
