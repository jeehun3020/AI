ðŸš€ Transformer-Based Sentiment Analysis Project

This repository contains my work for the Natural Language Processing (NLP) assignment focused on implementing Transformer-based sentiment analysis using BERT modules and a custom decoder architecture.

This project explores how modern Transformer architectures can be adapted for text understanding tasks and compares their performance with earlier RNN-based and CNN-based approaches.

ðŸ“Œ Project Overview

The goal of this project is to build a sentiment analysis model capable of classifying movie reviews from the IMDB dataset as positive or negative.

Key Features
	â€¢	ðŸ”¹ BERT Preprocessing + BERT Encoder from TensorFlow Hub
	â€¢	ðŸ”¹ Custom Transformer-based Decoder
	â€¢	ðŸ”¹ Fully end-to-end fine-tuning
	â€¢	ðŸ”¹ IMDB dataset (25,000 reviews)
	â€¢	ðŸ”¹ Achieved high accuracy (~0.83) on validation data

This project demonstrates how Transformer models outperform earlier architectures by capturing contextual meaning across entire sequences.

ðŸ§  Model Architecture

The final model consists of:
	1.	Input Layer â€” raw text
	2.	BERT Preprocessing Layer
	3.	BERT Encoder (trainable)
	4.	Transformer-Based Decoder
	5.	Sentiment Classification Head

The model produces a binary prediction representing positive or negative sentiment.

ðŸ§ª Training Results
	â€¢	Epochs: 8
	â€¢	Training Accuracy: steadily improved each epoch
	â€¢	Validation Accuracy: peaked around 82â€“83%
	â€¢	Test Accuracy: 0.8274

This confirms the performance boost gained from using a Transformer-based pipeline.

ðŸ“ˆ Future Improvements
	â€¢	Experiment with larger BERT variants (e.g., BERT-large, RoBERTa)
	â€¢	Utilize data augmentation for text
	â€¢	Incorporate learning-rate schedulers or warmup strategies
	â€¢	Compare against LSTM- or GRU-based baselines
